นี่คือแนวทางที่เป็นไปได้และเป็นที่ยอมรับในแวดวงวิชาการ สำหรับการประเมินผลโดยไม่มีผู้เชี่ยวชาญครับ:

วิธีที่ 1: การประเมินโดยตัวผู้วิจัยเองอย่างมีระบบ (Systematic Self-Evaluation)
วิธีนี้เป็นวิธีที่ตรงไปตรงมาและใช้กันบ่อยที่สุดในงานวิจัยระดับบัณฑิตศึกษา คือ ตัวคุณในฐานะผู้วิจัย คือผู้เชี่ยวชาญ (Proxy Expert) เอง เพราะคุณคือคนที่ศึกษาทฤษฎี DDD มาอย่างลึกซึ้งที่สุดในโครงการนี้ แต่การจะทำให้มันน่าเชื่อถือได้ ต้องทำอย่างมีหลักการชัดเจนครับ

วิธีการ:

สร้างเกณฑ์การให้คะแนน (Rubric) ที่ชัดเจน: ก่อนเริ่มประเมิน ให้สร้างตารางเกณฑ์การตัดสินใจที่ละเอียดมากๆ ว่าอะไรคือ "Entity ที่ถูกต้อง", "Aggregate ที่สมบูรณ์", "Business Rule ที่ครบถ้วน" โดยอ้างอิงจากตำราหรือแหล่งข้อมูล DDD ที่น่าเชื่อถือ
ประเมินอย่างสม่ำเสมอ: ใช้ Rubric เดียวกันนี้ในการตรวจผลลัพธ์จาก LLM ทุกชิ้น
ทดสอบความสอดคล้องของตัวเอง (Intra-rater reliability): ลองนำผลลัพธ์ชิ้นเดิมกลับมาประเมินซ้ำในเวลาที่ต่างกัน (เช่น เว้นระยะ 1 สัปดาห์) เพื่อดูว่าคุณยังให้คะแนนเหมือนเดิมหรือไม่ เป็นการพิสูจน์ว่าเกณฑ์ของคุณมีความนิ่ง
ยอมรับในข้อจำกัด: ในเล่มวิจัย ต้องเขียนระบุอย่างโปร่งใสว่าการประเมินผลนี้ทำโดยผู้วิจัย และถือเป็นหนึ่งในข้อจำกัดของงานวิจัย (Limitation)
ข้อดี: ทำได้จริง, ควบคุมได้ทั้งหมด
ข้อเสีย: อาจถูกตั้งคำถามเรื่องความเป็นกลาง (Bias)

วิธีที่ 2: การประเมินเชิงเปรียบเทียบ (Comparative / Relative Evaluation)
เปลี่ยนมุมมองจากการวัด "ความถูกต้องสมบูรณ์แบบ" มาเป็นการวัดว่า "แนวทางไหนดีกว่ากัน"

วิธีการ:

เปรียบเทียบระหว่าง Prompt: นี่คือสิ่งที่คุณวางแผนจะทำอยู่แล้ว! นำผลลัพธ์ที่ได้จาก Prompt แต่ละแบบ (Zero-shot, Few-shot, Chain-of-Thought) มาเทียบกันโดยใช้เกณฑ์ (Rubric) จากวิธีที่ 1 เพื่อหาว่าเทคนิคไหนให้ผลลัพธ์ที่ดีที่สุด
เปรียบเทียบระหว่าง Model: นำ Requirement เดียวกันไปทดลองกับ LLM หลายๆ ตัว (เช่น Llama 3, GPT-4o, Gemini, Claude 3) แล้วนำผลลัพธ์มาเปรียบเทียบกันเพื่อดูแนวโน้มว่าโมเดลประเภทไหนจัดการกับภาษาไทยและแนวคิด DDD ได้ดีกว่า
การวัดผล: ผลลัพธ์ที่ได้จะไม่ใช่ "โมเดลนี้แม่นยำ 85%" แต่จะเป็น "เทคนิค Few-shot ให้ผลลัพธ์ที่ดีกว่า Zero-shot ในการสกัด Business Rule อย่างมีนัยสำคัญ"
ข้อดี: ลดความจำเป็นในการมี "เฉลยที่สมบูรณ์แบบ", เป็นการประเมินที่มีความเป็นวิทยาศาสตร์สูง
ข้อเสีย: บอกได้แค่ว่าอะไรดีกว่า แต่บอกไม่ได้ว่าดีที่สุดนั้น "ดีพอ" แล้วหรือยัง

วิธีที่ 3: การประเมินผ่านงานขั้นต่อไป (Downstream Task Evaluation)
เป็นวิธีที่ทรงพลังและน่าเชื่อถือมาก คือการใช้ผลลัพธ์จาก LLM ไปทำงานอย่างอื่นต่อ แล้ววัดคุณภาพจากงานที่สองแทน

วิธีการ:

สร้างโค้ดเบื้องต้น (Code Generation): นำไฟล์ JSON ที่ LLM สกัดได้ มาสร้าง Prompt อีกชุดหนึ่งเพื่อสั่งให้ LLM อีกตัว (หรือตัวเดิม) สร้างเป็น Class Code เบื้องต้น (เช่น ภาษา C# หรือ Java) จากนั้นนำโค้ดที่ได้มาประเมิน
เกณฑ์วัดผล: โค้ดคอมไพล์ผ่านหรือไม่? โครงสร้างคลาสสอดคล้องกับหลักการ OOP หรือไม่? ความสัมพันธ์ระหว่างคลาสสมเหตุสมผลหรือไม่? (ซึ่งทั้งหมดนี้ประเมินง่ายกว่าการประเมิน DDD โดยตรง)
สร้างกรณีทดสอบ (Test Case Generation): นำส่วน "Business Rules" ใน JSON มาสร้าง Prompt เพื่อให้ LLM สร้างเป็น กรณีทดสอบในรูปแบบ Gherkin (Given/When/Then)
เกณฑ์วัดผล: กรณีทดสอบที่สร้างขึ้นมานั้นสมเหตุสมผล, ครอบคลุมเงื่อนไข และสามารถนำไปใช้พัฒนาต่อได้จริงหรือไม่?
ข้อดี: แสดงให้เห็นถึง "ประโยชน์ใช้สอยได้จริง" ของผลลัพธ์, มีเกณฑ์การตัดสินที่จับต้องได้ง่าย
ข้อเสีย: ต้องทำงานเพิ่มขึ้นอีกหนึ่งขั้นตอน

ข้อเสนอแนะ: ใช้แนวทางแบบผสมผสาน (Hybrid Approach)
วิธีที่ดีและรัดกุมที่สุดสำหรับงานวิจัยของคุณคือการใช้หลายวิธีร่วมกัน:

ใช้ "วิธีที่ 1 (การประเมินโดยผู้วิจัย)" เป็นแกนหลัก โดยสร้าง Rubric ที่ดีและโปร่งใสที่สุดเท่าที่จะทำได้ จากนั้น ใช้ "วิธีที่ 3 (การประเมินผ่านงานขั้นต่อไป)" มาเป็นตัวเสริมเพื่อพิสูจน์ให้เห็นว่าผลลัพธ์จาก LLM ของคุณไม่ได้ดีแค่ในทางทฤษฎี แต่ยังสามารถนำไปสร้างประโยชน์ในทางปฏิบัติ (เช่น สร้างโค้ด) ได้จริง